{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction :\n",
    "def initialisation(dimensions):\n",
    "    \n",
    "    param = {}\n",
    "    for l in range(1, len(dimensions)):\n",
    "        param['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l - 1])\n",
    "        param['b' + str(l)] = np.random.randn(dimensions[l], 1)\n",
    "\n",
    "    return param\n",
    "\n",
    "def forward_propagation(X, param):\n",
    "    activations = {'A0' : X}\n",
    "    L = len(param)//2\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        Z = param['W' + str(l)].dot(activations['A' + str(l-1)]) + param['b' + str(l)]\n",
    "        activations['A' + str(l)] = 1 / (1 + np.exp(-Z))\n",
    "        \n",
    "    return activations\n",
    "\n",
    "def back_propagation(Y, activations, param):\n",
    "    m = Y.shape[1]\n",
    "    L = len(param) // 2\n",
    "    \n",
    "    dZ = activations['A' + str(L)] - Y   \n",
    "    gradients = {} \n",
    "    \n",
    "    for l in reversed(range(1, L+1)):\n",
    "        gradients['dW' + str(l)] = 1/m * np.dot(dZ, activations['A' + str(l - 1)].T)\n",
    "        gradients['db' + str(l)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        if l > 1:\n",
    "            dZ = np.dot(param['W' + str(l)].T, dZ) * activations['A' + str(l-1)] * (1-activations['A' + str(l-1)])\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update(gradients, param, learning_rate):\n",
    "    \n",
    "    L = len(param) // 2\n",
    "    for l in range(1, L+1):\n",
    "        param['W' + str(l)] = param['W' + str(l)] - learning_rate * gradients['dW' + str(l)]\n",
    "        param['b' + str(l)] = param['b' + str(l)] - learning_rate * gradients['db' + str(l)]\n",
    "    \n",
    "    return param\n",
    "\n",
    "def predict(X, param):\n",
    "    activations = forward_propagation(X, param)\n",
    "    L = len(param) // 2\n",
    "    Af = activations['A'+str(L)]\n",
    "    return (Af >= 0.5)\n",
    "\n",
    "def normalisedAndReshape(set):    \n",
    "    return set.reshape(set.shape[0], -1)/set.max()\n",
    "\n",
    "def affichage_resultat(lossAcc):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(lossAcc[0], label='train loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(lossAcc[1], label='train acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def log_loss(Y, A):\n",
    "    e = 1e-15\n",
    "    return 1/len(Y) * np.sum(-Y * np.log(A + e) - (1 - Y) * np.log(1 - A + e))\n",
    "            \n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/trainset.hdf5', \"r\")\n",
    "    X_train = np.array(train_dataset[\"X_train\"][:]) # your train set features\n",
    "    y_train = np.array(train_dataset[\"Y_train\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/testset.hdf5', \"r\")\n",
    "    X_test = np.array(test_dataset[\"X_test\"][:]) # your train set features\n",
    "    y_test = np.array(test_dataset[\"Y_test\"][:]) # your train set labels\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test \n",
    "\n",
    "                     \n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction principale de train avec comparaison du test\n",
    "def neural_network(X, Y, hidden_layers = (2, 4, 8), learning_rate=0.1, n_iter = 1000, nb_affichage=1000):\n",
    "    # Initialisation des variables : --------------------------------------------------------------\n",
    "    dimensions = list(hidden_layers)\n",
    "    dimensions.insert(0, X.shape[0])\n",
    "    dimensions.append(Y.shape[0])\n",
    "    print(dimensions)\n",
    "    loss_acc = ([],[])\n",
    "    history_of = []\n",
    "    # gestion de l'affichage : --------------------------------------------------------------------\n",
    "    if n_iter <= nb_affichage:\n",
    "        x_sample = 1\n",
    "    else:\n",
    "        x_sample = int((n_iter/nb_affichage))\n",
    "    \n",
    "    \n",
    "    # Initialisation du trainning : ---------------------------------------------------------------\n",
    "    np.random.seed(0)\n",
    "    param = initialisation(dimensions) # initialisation des poids du modèle et du biais\n",
    "\n",
    "    # Aprendtissage : -----------------------------------------------------------------------------\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        activations = forward_propagation(X, param)\n",
    "        gradients = back_propagation(Y, activations, param)\n",
    "        param = update(gradients, param, learning_rate)\n",
    "        \n",
    "        if i % x_sample == 0 :\n",
    "            L = len(param) // 2\n",
    "            y_pred = predict(X, param)\n",
    "            loss_acc[0].append(log_loss(Y, activations['A'+str(L)]))\n",
    "            loss_acc[1].append(accuracy_score(Y.flatten(), y_pred.flatten()))       \n",
    "        \n",
    "\n",
    "    # Affichage des résultats : -------------------------------------------------------------------\n",
    "    affichage_resultat(loss_acc)\n",
    "    \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (4096, 1000) (1, 1000) (4096, 200) (1, 200) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Récupération des données :\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# Normalisation et flatten des données :\n",
    "normFlat_X_train = normalisedAndReshape(X_train)\n",
    "normFlat_X_test = normalisedAndReshape(X_test)\n",
    "\n",
    "normFlat_X_train = normFlat_X_train.T\n",
    "normFlat_X_test = normFlat_X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(\"\\n\", normFlat_X_train.shape, y_train.shape, normFlat_X_test.shape, y_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4096, 64, 64, 64, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 268/4096 [00:06<01:33, 41.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# lancement de l'apprentissage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mneural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormFlat_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_affichage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mneural_network\u001b[1;34m(X, Y, hidden_layers, learning_rate, n_iter, nb_affichage)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Aprendtissage : -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_iter)):\n\u001b[1;32m---> 23\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m back_propagation(Y, activations, param)\n\u001b[0;32m     25\u001b[0m     param \u001b[38;5;241m=\u001b[39m update(gradients, param, learning_rate)\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, param)\u001b[0m\n\u001b[0;32m     13\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(param)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, L\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     Z \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)]\u001b[38;5;241m.\u001b[39mdot(activations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]) \u001b[38;5;241m+\u001b[39m param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)]\n\u001b[0;32m     17\u001b[0m     activations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mZ))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lancement de l'apprentissage\n",
    "param = neural_network(normFlat_X_train, y_train, hidden_layers = (64, 64, 64), learning_rate=0.1, n_iter = 4096, nb_affichage=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.54 with Test set\n",
      "Accuracy score of 0.981 with Training set\n"
     ]
    }
   ],
   "source": [
    "def test_neural_network(X, Y, param, name_set):\n",
    "    # Aprendtissage : -----------------------------------------------------------------------------\n",
    "    y_pred = predict(X, param)\n",
    "    acc = (accuracy_score(Y.flatten(), y_pred.flatten()))       \n",
    "    print(f\"Accuracy score of {acc} with {name_set}\")\n",
    "    \n",
    "test_neural_network(normFlat_X_test, y_test, param, \"Test set\")\n",
    "test_neural_network(normFlat_X_train, y_train, param, \"Training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score of 0.555 with Test set\n",
    "Accuracy score of 1.0 with Training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
